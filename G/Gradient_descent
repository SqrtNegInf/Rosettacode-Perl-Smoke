#!/usr/local/bin/perl
#u# http://rosettacode.org/wiki/Gradient_descent
#c# 2019-11-01 >RC
#p# OK

use strict;
use warnings;
use feature 'say';

use bignum;
use List::Util qw(sum);

sub steepestDescent {
    my($alpha, $tolerance, @x) = @_;
    my $N = @x; 
    my $h = $tolerance;
    my $g0 = g(@x);         # Initial estimate of result
    my @fi = gradG($h, @x); # Calculate initial gradient

    # Calculate initial norm
#    my $delG = 0;
#    $delG += $fi[$_]**2 for 0..$N-1;
#    my $b = $alpha / sqrt($delG);
    my $b = $alpha / (my $delG = sqrt sum map { $fi[$_]**2 } 0..$N-1);

    while ( $delG > $tolerance ) {
       #  Calculate next value
       $x[$_] -= $b * $fi[$_] for 0..$N-1;
       $h /= 2;
       @fi = gradG($h, @x);    # Calculate next gradient
       # Calculate next norm
       $delG = 0;
       $delG += $fi[$_]**2 for 0..$N-1;
       $b = $alpha / sqrt($delG);
       my $g1 = g(@x);   # Calculate next value
       $g1 > $g0 ? ($alpha /= 2) : ($g0 = $g1);  # Adjust parameter
    }
    @x
}

# Provides a rough calculation of gradient g(x)
sub gradG {
    my($h, @x) = @_;
    my $N = @x; 
    my @y = @x;
    my $g0 = g(@x);
#    my @z;
#    for (0..$N-1) { 
#        $y[$_] += $h; 
#        $z[$_] = (g(@y) - $g0) / $h 
#    }
#    @z
    map { $y[$_] += $h; (g(@y) - $g0) / $h } 0..$N-1
}

# Function for which minimum is to be found
sub g { my(@x) = @_; ($x[0]-1)**2 * exp(-$x[1]**2) + $x[1]*($x[1]+2) * exp(-2*$x[0]**2) };

my $tolerance = 0.0000001; 
my $alpha     = 0.01;
my @x         = <0.1 -1>; # Initial guess of location of minimum
say my $result = sprintf "The minimum is at x[0] = %.6f, x[1] = %.6f", steepestDescent($alpha, $tolerance, @x);

use Test::More;
is($result, 'The minimum is at x[0] = 0.107653, x[1] = -1.223370');
done_testing;
